---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Zhiyuan Ma is a postdoctoral fellow in the Department of Electronic Engineering at Tsinghua University and a recipient of the National Natural Science Foundation of China Youth Fund. His co-supervisor is Professor Zhou Bowen. As the person in charge, he presided over a number of national natural science projects, postdoctoral general projects, and postdoctoral national funding plan projects, and also participated in a number of major projects of the Ministry of Science and Technology in 2030. He was also a member of ACL, ACM, CCF professional member, member of the Beijing Zhiyuan BAAI-Qingyuan research group, and reviewer of top international journals and conferences such as TNNLS, ICLR, ICML, NeurIPS, ACL, EMNLP, COLING, NAACL, AAAI, ECAI, CIKM, etc. He received his Ph.D. from Huazhong University of Science and Technology and will graduate from the School of Computer Science of Huazhong University of Science and Technology one year ahead of schedule in June 2023 (the first doctoral student to graduate ahead of schedule). During his doctoral studies, he has won many honors, including Outstanding Doctoral Graduate, Outstanding Graduation Thesis, National Doctoral Scholarship, Guanghua Scholarship, BIGO Enterprise Scholarship, Outstanding Graduate Student Cadre, Three Good Graduate Student, Zhiyin Pilot Student, Zhiyin Pillar Student Academic Research Award (the only one in the college), and won the Best Paper of the First Annual Academic Conference of HUST-CS in 2022. During his direct doctoral studies, he focused on natural language processing, multimodal semantic understanding, large-scale pre-training of vision-language, task-based dialogue systems, and controllable generation. His main work has been published in the top international conferences of artificial intelligence and natural language processing, such as NeurIPS, ACL, EMNLP, AAAI, ACM MM, and COLING, as the first author. His work in the field of multimodal pre-training, CMAL, has been cited and positively evaluated by the latest work of LeCun (Turing Award winner and Facebook Chief AI Scientist). This work innovatively proposed the concept and pre-training method of cross-modal associative learning, and made new breakthroughs in modality-aligned vision-language pre-training.


# üî• News
- *2024.09*: &nbsp;üéâ Three papers are accepted by NeurIPS 2024.
- *2024.08*: &nbsp;üéâ Received the news of the National Natural Science Foundation Youth Project FundingÔºÅ
- *2024.07*: &nbsp;üéâ One paper is accepted by ACM MM 2024.
- *2023.12*: &nbsp;üéâ Three papers are accepted by AAAI 2024.
- *2022.11*: &nbsp;üéâ One paper is accepted by AAAI 2023 as **Oral** paper.
- *2022.06*: &nbsp;üéâ One paper is accepted by ACM MM 2022 as **Oral** paper.
- *2022.02*: &nbsp;üéâ One main track paper is accepted by ACL 2022.
- *2021.08*: &nbsp;üéâ Two papers are accepted by EMNLP 2021, one of which is the main track paper and the other is findings.

# üßë‚Äçüè´ Projects
- *2024.08*: &nbsp; National Natural Science Foundation (NSFC) Youth Project (No.62406161), ÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÈùíÂπ¥È°πÁõÆ.
- *2023.12*: &nbsp; Nationally Funded Postdoctoral Researcher Program (No.GZB20230347), ÂõΩÂÆ∂ËµÑÂä©ÂçöÂ£´ÂêéÁ†îÁ©∂‰∫∫ÂëòËÆ°ÂàíÔºàBÊ°£Ôºâ.
- *2023.11*: &nbsp; Fellowship from the China Postdoctoral Science Foundation (No.2023M741950), ‰∏≠ÂõΩÂçöÂ£´ÂêéÁßëÂ≠¶Âü∫ÈáëÁ¨¨74ÊâπÈù¢‰∏äËµÑÂä©.

# üìë Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='pub_images/neural-rdm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Neural Residual Diffusion Models for Deep Scalable Vision Generation]([https://arxiv.org/pdf/2406.13215])

**Zhiyuan Ma**, Liangliang Zhao, Biqing Qi, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='pub_images/safe-sd-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking]([https://arxiv.org/pdf/2407.13188])

**Zhiyuan Ma**, Guoli Jia, Biqing Qi, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/lmd-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LMD: faster image reconstruction with latent masking diffusion]([https://ojs.aaai.org/index.php/AAAI/article/view/28209])

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/adapedit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing]([https://ojs.aaai.org/index.php/AAAI/article/view/28210])

**Zhiyuan Ma**, Guoli Jia, Bowen Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='pub_images/gemkr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generative multi-modal knowledge retrieval with large language models]([https://ojs.aaai.org/index.php/AAAI/article/view/29837])

Xinwei Long, Jiali Zeng, Fandong Meng, **Zhiyuan Ma**, Kaiyan Zhang, Bowen Zhou, Jie Zhou

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='pub_images/hybrid-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HybridPrompt: bridging language models and human priors in prompt tuning for visual question answering]([https://ojs.aaai.org/index.php/AAAI/article/view/26569])

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2022</div><img src='pub_images/cmal-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Cmal: A novel cross-modal associative learning framework for vision-language pre-training]([https://dl.acm.org/doi/abs/10.1145/3503161.3548292])

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2022</div><img src='pub_images/glaf-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GLAF: global-to-local aggregation and fission network for semantic level fact verification]([https://aclanthology.org/2022.coling-1.155.pdf])

**Zhiyuan Ma**, Zhihuan Yu, Jianjun Li, Guohui Li

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2022</div><img src='pub_images/unitranser-3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[UniTranSeR: A unified transformer semantic representation framework for multimodal task-oriented dialog system]([https://aclanthology.org/2022.acl-long.9.pdf])

**Zhiyuan Ma**, Jianjun Li, Guohui Li, Yongjing Cheng

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2021</div><img src='pub_images/irnet-2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Intention reasoning network for multi-domain end-to-end task-oriented dialogue]([https://aclanthology.org/2021.emnlp-main.174.pdf])

**Zhiyuan Ma**, Jianjun Li, Zezheng Zhang, Guohui Li, Yongjing Cheng

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>



# üéñ Honors and Awards
- *2023.06* One of the best papers in the first HUST-CS annual academic conference.
- *2023.05* Zhiyin Pillar Student Academic Research Award (the only one in the college).
- *2022.10* Doctoral National Scholarship Honors, Zhiyin Student Scholarship and Three Good Graduate Student Honors.
- *2021.10* BIGO Enterprise Scholarship, Guanghua Scholarship, Outstanding Graduate Student Leader.

# üéì Educations
- *2023.07 - 2025.07 (now)*, Postdoctoral fellow, Tsinghua University, Beijing, China. 
- *2019.09 - 2023.06*, PhD student, Huazhong University of Science and Technology, Wuhan, Hubei, China. 

# üíª Internships
- *None*
